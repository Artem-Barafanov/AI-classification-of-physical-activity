{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\n",
    "    \"timestamp\",\n",
    "    \"activity_id\",  \n",
    "    \"heart_rate\",\n",
    "    \"IMU_hand_temperature\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_hand_3D_gyroscope_data_1\",\n",
    "    \"IMU_hand_3D_gyroscope_data_2\",\n",
    "    \"IMU_hand_3D_gyroscope_data_3\",\n",
    "    \"IMU_hand_3D_magnetometer_data_1\",\n",
    "    \"IMU_hand_3D_magnetometer_data_2\",\n",
    "    \"IMU_hand_3D_magnetometer_data_3\",\n",
    "    \"IMU_hand_orientation_1\",\n",
    "    \"IMU_hand_orientation_2\",\n",
    "    \"IMU_hand_orientation_3\",\n",
    "    \"IMU_hand_orientation_4\",\n",
    "    \"IMU_chest_temperature\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_chest_3D_gyroscope_data_1\",\n",
    "    \"IMU_chest_3D_gyroscope_data_2\",\n",
    "    \"IMU_chest_3D_gyroscope_data_3\",\n",
    "    \"IMU_chest_3D_magnetometer_data_1\",\n",
    "    \"IMU_chest_3D_magnetometer_data_2\",\n",
    "    \"IMU_chest_3D_magnetometer_data_3\",\n",
    "    \"IMU_chest_orientation_1\",\n",
    "    \"IMU_chest_orientation_2\",\n",
    "    \"IMU_chest_orientation_3\",\n",
    "    \"IMU_chest_orientation_4\",\n",
    "    \"IMU_ankle_temperature\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_1\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_2\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_3\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_1\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_2\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_3\",\n",
    "    \"IMU_ankle_orientation_1\",\n",
    "    \"IMU_ankle_orientation_2\",\n",
    "    \"IMU_ankle_orientation_3\",\n",
    "    \"IMU_ankle_orientation_4\",\n",
    "]\n",
    "\n",
    "# for_gen = [    \"IMU_hand_3D_acceleration_data_16g\",\n",
    "#     \"IMU_hand_3D_acceleration_data_16g\",\n",
    "#     \"IMU_hand_3D_acceleration_data_16g\",\n",
    "#     \"IMU_hand_3D_acceleration_data_6g\",\n",
    "#     \"IMU_hand_3D_acceleration_data_6g\",\n",
    "#     \"IMU_hand_3D_acceleration_data_6g\",\n",
    "#     \"IMU_hand_3D_gyroscope_data\",\n",
    "#     \"IMU_hand_3D_gyroscope_data\",\n",
    "#     \"IMU_hand_3D_gyroscope_data\",\n",
    "#     \"IMU_hand_3D_magnetometer_data\",\n",
    "#     \"IMU_hand_3D_magnetometer_data\",\n",
    "#     \"IMU_hand_3D_magnetometer_data\",\n",
    "#     \"IMU_hand_orientation\",\n",
    "#     \"IMU_hand_orientation\",\n",
    "#     \"IMU_hand_orientation\",]\n",
    "\n",
    "# for el in for_gen:\n",
    "#     print(f\"\\\"{el.replace('hand', 'ankle')}\\\",\")\n",
    "len(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1 = pd.read_csv(\"Protocol\\\\subject101.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person2 = pd.read_csv(\"Protocol\\\\subject102.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person3 = pd.read_csv(\"Protocol\\\\subject103.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person4 = pd.read_csv(\"Protocol\\\\subject104.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person5 = pd.read_csv(\"Protocol\\\\subject105.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person6 = pd.read_csv(\"Protocol\\\\subject106.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person7 = pd.read_csv(\"Protocol\\\\subject107.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person8 = pd.read_csv(\"Protocol\\\\subject108.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person9 = pd.read_csv(\"Protocol\\\\subject109.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "\n",
    "person1_additional = pd.read_csv(\"Optional\\\\subject101.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person5_additional = pd.read_csv(\"Optional\\\\subject105.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person6_additional = pd.read_csv(\"Optional\\\\subject106.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person8_additional = pd.read_csv(\"Optional\\\\subject108.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person9_additional = pd.read_csv(\"Optional\\\\subject109.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [person1, person2, person3, person4, person5, person6, person7, person8, person9, person1_additional, person5_additional, person6_additional, person8_additional, person9_additional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([col for col in person1.columns if person1[col].isna().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in person1.columns:\n",
    "    if el not in [col for col in person1.columns if person1[col].isna().any()]:\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in person1.columns:\n",
    "    print(col, person1[col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_for_drop = [col for col in person1.columns if col.endswith(tuple(f\"orientation_{i}\" for i in range(1, 5)))]\n",
    "col_for_drop += [col for col in person1.columns if col.endswith(tuple(f\"acceleration_data_6g_{i}\" for i in range(1, 4)))]\n",
    "col_for_drop += [\"timestamp\"]\n",
    "person1.drop(columns=col_for_drop)\n",
    "len(col_for_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hr_1, hr_2 = 104, 104\n",
    "first, second = 0, 10\n",
    "\n",
    "for df in [person1, person2, person3, person4, person5, person6, person7, person8, person9]:\n",
    "    for i in range(11, len(df)):\n",
    "        if not pd.isna(df[\"heart_rate\"][i]):\n",
    "            # print(\"number is found\", person1[\"heart_rate\"][i], first, second, hr_1, hr_2)\n",
    "            df.loc[first + 1:second, \"heart_rate\"] = df.loc[first + 1:second, \"heart_rate\"].fillna((hr_1 + hr_2) / 2)\n",
    "            first, second = second, i\n",
    "            hr_1, hr_2 = hr_2, df[\"heart_rate\"][i]\n",
    "    \n",
    "# df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # получаем позиции (integer index) всех ненулевых значений\n",
    "    hr_positions = np.flatnonzero(df['heart_rate'].notna())\n",
    "    # если мало точек — ничего не заполняем\n",
    "    if len(hr_positions) < 2:\n",
    "        continue\n",
    "\n",
    "    # инициализируем первые две точки\n",
    "    first_idx, second_idx = hr_positions[0], hr_positions[1]\n",
    "    hr_1 = df['heart_rate'].iat[first_idx]\n",
    "    hr_2 = df['heart_rate'].iat[second_idx]\n",
    "\n",
    "    # для каждой следующей известной точки\n",
    "    for pos in hr_positions[2:]:\n",
    "        # заполняем промежуток [first_idx+1 : second_idx] средним\n",
    "        fill_val = (hr_1 + hr_2) / 2\n",
    "        df.loc[first_idx + 1 : second_idx, 'heart_rate'] = (\n",
    "            df.loc[first_idx + 1 : second_idx, 'heart_rate']\n",
    "              .fillna(fill_val)\n",
    "        )\n",
    "        # сдвигаем «окно»\n",
    "        first_idx, second_idx = second_idx, pos\n",
    "        hr_1, hr_2 = hr_2, df['heart_rate'].iat[pos]\n",
    "\n",
    "    # заполняем все NaN после последней известной точки\n",
    "    df.loc[second_idx + 1 :, 'heart_rate'] = (\n",
    "        df.loc[second_idx + 1 :, 'heart_rate']\n",
    "          .fillna(hr_2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1[\"heart_rate\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1[\"heart_rate\"].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = person1.drop(columns=col_for_drop)\n",
    "df.dropna(inplace=True)\n",
    "df = df[df[\"activity_id\"] != 0]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"activity_id\"])\n",
    "y = df[\"activity_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24]\n",
    "len(all_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [person1, person2, person3, person4, person5, person6, person7, person8, person9, person1_additional, person5_additional, person6_additional, person8_additional, person9_additional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocole = pd.concat([person1, person2, person3, person4, person5, person6, person7, person8, person9], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocole = df_protocole.drop(columns=col_for_drop)\n",
    "df_protocole = df_protocole.dropna()\n",
    "df_protocole = df_protocole[df_protocole[\"activity_id\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_protocole[df_protocole[\"activity_id\"] == 1]\n",
    "df_2 = df_protocole[df_protocole[\"activity_id\"] == 2]\n",
    "df_3 = df_protocole[df_protocole[\"activity_id\"] == 3]\n",
    "df_4 = df_protocole[df_protocole[\"activity_id\"] == 4]\n",
    "df_5 = df_protocole[df_protocole[\"activity_id\"] == 5]\n",
    "df_6 = df_protocole[df_protocole[\"activity_id\"] == 6]\n",
    "df_7 = df_protocole[df_protocole[\"activity_id\"] == 7]\n",
    "df_9 = df_protocole[df_protocole[\"activity_id\"] == 9]\n",
    "df_10 = df_protocole[df_protocole[\"activity_id\"] == 10]\n",
    "df_11 = df_protocole[df_protocole[\"activity_id\"] == 11]\n",
    "df_12 = df_protocole[df_protocole[\"activity_id\"] == 12]\n",
    "df_13 = df_protocole[df_protocole[\"activity_id\"] == 13]\n",
    "df_16 = df_protocole[df_protocole[\"activity_id\"] == 16]\n",
    "df_17 = df_protocole[df_protocole[\"activity_id\"] == 17]\n",
    "df_18 = df_protocole[df_protocole[\"activity_id\"] == 18]\n",
    "df_19 = df_protocole[df_protocole[\"activity_id\"] == 19]\n",
    "df_20 = df_protocole[df_protocole[\"activity_id\"] == 20]\n",
    "df_24 = df_protocole[df_protocole[\"activity_id\"] == 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_by_id = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_9, df_10, df_11, df_12, df_12, df_13, df_16, df_17, df_18, df_19, df_20, df_24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame\n",
    "test_df = pd.DataFrame\n",
    "\n",
    "for df in df_grouped_by_id:\n",
    "    if len(df) != 0:\n",
    "        split_idx_train = int(len(df) * 0.1)\n",
    "        split_idx_test = int(len(df) * 0.02)\n",
    "        train_df = pd.concat([train_df, df.iloc[:split_idx_train]])\n",
    "        test_df = pd.concat([test_df, df.iloc[split_idx_train : split_idx_train + split_idx_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(columns=col_for_drop)\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all[df_all[\"activity_id\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_value = df_all[\"activity_id\"].unique()\n",
    "print(sorted(uniq_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_activities = set()\n",
    "for df in df_list:\n",
    "    for el in df[\"activity_id\"].unique():\n",
    "        type_of_activities.add(el)\n",
    "\n",
    "len(type_of_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"activity_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.dropna(inplace=True)\n",
    "df_all = df_all[df_all[\"activity_id\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocole = df_protocole.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_protocole.drop(columns=[\"activity_id\"])\n",
    "y = df_protocole[\"activity_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all.drop(columns=[\"activity_id\"])\n",
    "y = df_all[\"activity_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем строки с activity_id == 0 в исходном DataFrame\n",
    "df_filtered = df_all[df_all[\"activity_id\"] != 0]\n",
    "\n",
    "# Пересоздаем X и y после фильтрации\n",
    "X = df_filtered.drop(columns=[\"activity_id\"])\n",
    "y = df_filtered[\"activity_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_time_series_split(X, y, activity_col='activity_id', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits time-series data into train/test sets while:\n",
    "    - Preserving temporal order within each `activity_id`.\n",
    "    - Ensuring all activity types appear in both splits.\n",
    "    \"\"\"\n",
    "    X_train, X_test = pd.DataFrame(), pd.DataFrame()\n",
    "    y_train, y_test = pd.Series(dtype=y.dtype), pd.Series(dtype=y.dtype)\n",
    "\n",
    "    # Split each activity group sequentially\n",
    "    for activity in X[activity_col].unique():  # Read activity_id from X, not y!\n",
    "        # Filter data for this activity\n",
    "        mask = (X[activity_col] == activity)\n",
    "        X_activity = X[mask]\n",
    "        y_activity = y[mask]\n",
    "\n",
    "        # Split chronologically (no shuffling)\n",
    "        split_idx = int(len(X_activity) * (1 - test_size))\n",
    "        if split_idx == 0:  # Handle tiny groups (at least 1 sample in train)\n",
    "            split_idx = 1\n",
    "        if split_idx >= len(X_activity):  # Avoid empty test set\n",
    "            split_idx = len(X_activity) - 1\n",
    "\n",
    "        # Append splits\n",
    "        X_train = pd.concat([X_train, X_activity.iloc[:split_idx]])\n",
    "        X_test = pd.concat([X_test, X_activity.iloc[split_idx:]])\n",
    "        y_train = pd.concat([y_train, y_activity.iloc[:split_idx]])\n",
    "        y_test = pd.concat([y_test, y_activity.iloc[split_idx:]])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "# Usage\n",
    "X_train, X_test, y_train, y_test = stratified_time_series_split(X, y, activity_col='activity_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Test Set Report (may contain unseen classes):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.11      0.29      0.16    165111\n",
      "           2       0.08      0.08      0.08    161180\n",
      "           3       0.00      0.00      0.00    167391\n",
      "           4       0.17      0.46      0.25    224304\n",
      "           5       0.00      0.00      0.00     95631\n",
      "           6       0.00      0.00      0.00    163302\n",
      "           7       0.00      0.00      0.00    184444\n",
      "          12       0.02      0.04      0.03    101219\n",
      "          13       0.42      0.25      0.32     89973\n",
      "          16       0.18      0.13      0.15    152101\n",
      "          17       0.32      0.24      0.27    214409\n",
      "          24       0.00      0.00      0.00     47559\n",
      "\n",
      "    accuracy                           0.15   1766624\n",
      "   macro avg       0.11      0.12      0.10   1766624\n",
      "weighted avg       0.12      0.15      0.12   1766624\n",
      "\n",
      "\n",
      "Filtered Test Set Report (only classes seen in training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.29      0.28    165111\n",
      "           2       0.10      0.08      0.09    161180\n",
      "           3       0.00      0.00      0.00    167391\n",
      "           4       0.21      0.46      0.29    224304\n",
      "          12       0.02      0.04      0.03    101219\n",
      "          13       0.51      0.25      0.34     89973\n",
      "          16       0.18      0.13      0.15    152101\n",
      "          17       0.32      0.24      0.27    214409\n",
      "\n",
      "    accuracy                           0.20   1275688\n",
      "   macro avg       0.20      0.18      0.18   1275688\n",
      "weighted avg       0.20      0.20      0.19   1275688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1) Get unique classes from training data\n",
    "train_classes = np.unique(y_train)\n",
    "\n",
    "# 2) Initialize LabelEncoder only on train_classes\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "\n",
    "# 3) Encode y_train\n",
    "y_train_enc = le.transform(y_train)\n",
    "\n",
    "# 4) Filter test set to only include classes present in training\n",
    "test_mask = np.isin(y_test, train_classes)\n",
    "X_test_filt = X_test[test_mask]\n",
    "y_test_filt = y_test[test_mask]\n",
    "\n",
    "# 5) Create preprocessing pipeline with RobustScaler\n",
    "# (Assuming all columns need scaling - adjust if you have categorical features)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', RobustScaler(), list(range(X_train.shape[1])))  # Scale all features\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 6) Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=20,\n",
    "        gamma=0.1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 7) Train the pipeline\n",
    "pipeline.fit(X_train, y_train_enc)\n",
    "\n",
    "# 8) Predictions (automatically applies scaling)\n",
    "# Note: We predict on both filtered and unfiltered test sets for comparison\n",
    "y_pred_enc = pipeline.predict(X_test)  # On full test set (may contain unseen classes)\n",
    "y_pred_enc_filt = pipeline.predict(X_test_filt)  # On filtered test set\n",
    "\n",
    "# 9) Convert back to original labels\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "y_pred_filt = le.inverse_transform(y_pred_enc_filt)\n",
    "\n",
    "# 10) Evaluation\n",
    "print(\"Full Test Set Report (may contain unseen classes):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nFiltered Test Set Report (only classes seen in training):\")\n",
    "print(classification_report(y_test_filt, y_pred_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1) Filter labels present in y_train\n",
    "train_classes = np.unique(y_train)\n",
    "\n",
    "# 2) LabelEncoder fit only on train_classes\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "\n",
    "# 3) Encode y_train\n",
    "y_train_enc = le.transform(y_train)\n",
    "\n",
    "# 4) Filter test set (only labels seen in training)\n",
    "mask = np.isin(y_test, train_classes)\n",
    "X_test_filt = X_test[mask]\n",
    "y_test_filt = y_test[mask]\n",
    "y_test_enc = le.transform(y_test_filt)\n",
    "\n",
    "# 5) Define XGBClassifier (without fixed params)\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],           # Number of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],         # Step size shrinkage\n",
    "    'max_depth': [3, 6, 9],                    # Tree depth\n",
    "    'subsample': [0.8, 1.0],                   # % of samples per tree\n",
    "    # 'colsample_bytree': [0.8, 1.0],            # % of features per tree\n",
    "    # 'gamma': [0, 0.1],                         # Min loss reduction for split\n",
    "}\n",
    "\n",
    "# 7) GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',         # Or 'f1_macro' for multiclass\n",
    "    cv=3,                      # 3-fold cross-validation\n",
    "    n_jobs=-1,                 # Use all CPU cores\n",
    "    verbose=2                  # Print progress\n",
    ")\n",
    "\n",
    "# 8) Fit GridSearch on training data\n",
    "grid_search.fit(X_train, y_train_enc)\n",
    "\n",
    "# 9) Best model and params\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 10) Predictions (using best model)\n",
    "y_pred_enc = best_model.predict(X_test_filt)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# 11) Evaluation\n",
    "print(classification_report(y_test_filt, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Удаление строк с activity_id == 0 ДО разделения\n",
    "df_filtered = df_all[df_all[\"activity_id\"] != 0]\n",
    "\n",
    "# 2. Разделение на фичи и целевую переменную\n",
    "X = df_filtered.drop(columns=[\"activity_id\"])\n",
    "y = df_filtered[\"activity_id\"]\n",
    "\n",
    "# 3. Разделение на train/test\n",
    "split_idx = int(len(df_filtered) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# 4. Проверка размеров данных\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "# Если X_test пустой (например, split_idx == len(df_filtered)), добавьте:\n",
    "if X_test.shape[0] == 0:\n",
    "    raise ValueError(\"Тестовый набор пуст. Увеличьте размер данных или уменьшите процент разделения\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 31)) while a minimum of 1 is required by DecisionTreeClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_enc)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 3. Предсказание на тесте\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m y_pred_enc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_enc)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 4. Оценка модели\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:530\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    529\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 530\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    532\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:489\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    498\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    499\u001b[0m ):\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\maksson\\algorithms_venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 31)) while a minimum of 1 is required by DecisionTreeClassifier."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ваше разделение данных\n",
    "split_idx = int(len(df_all) * 0.8)  # Исправлено: len(df_all) вместо len(df)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# 1. Кодируем метки, если activity_id - строковые категории\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# 2. Создаем и обучаем модель\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=10,            # Оптимизируйте этот параметр\n",
    "    min_samples_leaf=10,    # Минимум 10 образцов в листе\n",
    "    criterion='entropy',       # Критерий разделения (gini/entropy)\n",
    "    random_state=42,\n",
    "    splitter=\"best\",\n",
    ")\n",
    "\n",
    "# model = RandomForestClassifier(\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     max_depth=10,      # Maximum tree depth\n",
    "#     n_jobs=-1,\n",
    "#     max_samples=0.8,\n",
    "#     random_state=42    # Reproducibility\n",
    "# )\n",
    "\n",
    "model.fit(X_train, y_train_enc)\n",
    "\n",
    "# 3. Предсказание на тесте\n",
    "y_pred_enc = model.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# 4. Оценка модели\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Матрица ошибок\n",
    "# plt.figure(figsize=(10,7))\n",
    "# cm = confusion_matrix(y_test_enc, y_pred_enc)\n",
    "# disp = ConfusionMatrixDisplay(\n",
    "#     confusion_matrix=cm,\n",
    "#     display_labels=le.classes_\n",
    "# )\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n",
    "# \n",
    "# Визуализация дерева (для небольших деревьев)\n",
    "# plt.figure(figsize=(20,12))\n",
    "# plot_tree(\n",
    "#     model,\n",
    "#     filled=True,\n",
    "#     feature_names=X.columns.tolist(),  # Используем настоящие имена фичей\n",
    "#     class_names=le.classes_,\n",
    "#     rounded=True,\n",
    "#     proportion=True,\n",
    "#     max_depth=3  # Показываем первые 3 уровня\n",
    "# )\n",
    "# plt.title(\"Decision Tree Structure\")\n",
    "# plt.show()\n",
    "\n",
    "# # Важность признаков\n",
    "# importances = model.feature_importances_\n",
    "# features = X.columns\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.barh(features, importances)\n",
    "# plt.xlabel(\"Importance Score\")\n",
    "# plt.title(\"Feature Importances\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score  # Исправленный импорт\n",
    "import numpy as np\n",
    "\n",
    "# Определяем модель\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Создаем стратегию кросс-валидации\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Задаем метрику для оптимизации с правильным параметром\n",
    "scorer = make_scorer(f1_score, average='weighted')  # Исправлено здесь\n",
    "\n",
    "# Остальной код остается без изменений\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 3, 5, 7, 9, 11, 15, 20, 25],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20, 0.01, 0.05, 0.1],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10, 15, 20, 0.01, 0.05, 0.1],\n",
    "    'max_features': [None, 'sqrt', 'log2', 0.5, 0.7, 0.9, 1.0],\n",
    "    'class_weight': [None, 'balanced', {0:1, 1:2}, {0:1, 1:3}],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1, 0.2],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.02, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Сколько комбинаций проверить\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Запускаем поиск\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшие параметры\n",
    "print(\"Лучшие параметры:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Лучшая модель\n",
    "best_dt = random_search.best_estimator_\n",
    "\n",
    "# Оценка на тестовых данных\n",
    "y_pred = best_dt.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithms_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
