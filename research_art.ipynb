{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"timestamp\",\n",
    "    \"activity_id\", \n",
    "    \"heart_rate\",\n",
    "    \"IMU_hand_temperature\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_hand_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_hand_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_hand_3D_gyroscope_data_1\",\n",
    "    \"IMU_hand_3D_gyroscope_data_2\",\n",
    "    \"IMU_hand_3D_gyroscope_data_3\",\n",
    "    \"IMU_hand_3D_magnetometer_data_1\",\n",
    "    \"IMU_hand_3D_magnetometer_data_2\",\n",
    "    \"IMU_hand_3D_magnetometer_data_3\",\n",
    "    \"IMU_hand_orientation_1\",\n",
    "    \"IMU_hand_orientation_2\",\n",
    "    \"IMU_hand_orientation_3\",\n",
    "    \"IMU_hand_orientation_4\",\n",
    "    \"IMU_chest_temperature\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_chest_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_chest_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_chest_3D_gyroscope_data_1\",\n",
    "    \"IMU_chest_3D_gyroscope_data_2\",\n",
    "    \"IMU_chest_3D_gyroscope_data_3\",\n",
    "    \"IMU_chest_3D_magnetometer_data_1\",\n",
    "    \"IMU_chest_3D_magnetometer_data_2\",\n",
    "    \"IMU_chest_3D_magnetometer_data_3\",\n",
    "    \"IMU_chest_orientation_1\",\n",
    "    \"IMU_chest_orientation_2\",\n",
    "    \"IMU_chest_orientation_3\",\n",
    "    \"IMU_chest_orientation_4\",\n",
    "    \"IMU_ankle_temperature\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_1\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_2\",\n",
    "    \"IMU_ankle_3D_acceleration_data_16g_3\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_1\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_2\",\n",
    "    \"IMU_ankle_3D_acceleration_data_6g_3\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_1\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_2\",\n",
    "    \"IMU_ankle_3D_gyroscope_data_3\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_1\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_2\",\n",
    "    \"IMU_ankle_3D_magnetometer_data_3\",\n",
    "    \"IMU_ankle_orientation_1\",\n",
    "    \"IMU_ankle_orientation_2\",\n",
    "    \"IMU_ankle_orientation_3\",\n",
    "    \"IMU_ankle_orientation_4\"\n",
    "]\n",
    "\n",
    "col_for_drop = [col for col in column_names if col.endswith(tuple(f\"orientation_{i}\" for i in range(1, 5)))]\n",
    "col_for_drop += [col for col in column_names if col.endswith(tuple(f\"acceleration_data_6g_{i}\" for i in range(1, 4)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1 = pd.read_csv(\"Protocol/subject101.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person2 = pd.read_csv(\"Protocol/subject102.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person3 = pd.read_csv(\"Protocol/subject103.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person4 = pd.read_csv(\"Protocol/subject104.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person5 = pd.read_csv(\"Protocol/subject105.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person6 = pd.read_csv(\"Protocol/subject106.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person7 = pd.read_csv(\"Protocol/subject107.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person8 = pd.read_csv(\"Protocol/subject108.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person9 = pd.read_csv(\"Protocol/subject109.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "\n",
    "person1_additional = pd.read_csv(\"Optional/subject101.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person5_additional = pd.read_csv(\"Optional/subject105.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person6_additional = pd.read_csv(\"Optional/subject106.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person8_additional = pd.read_csv(\"Optional/subject108.dat\", delimiter=\" \", names=column_names, header=None)\n",
    "person9_additional = pd.read_csv(\"Optional/subject109.dat\", delimiter=\" \", names=column_names, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Заполнение пульса:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [person1, person2, person3, person4, person5, person6, person7, person8, person9, \n",
    "           person1_additional, person5_additional, person6_additional, person8_additional, person9_additional]\n",
    "\n",
    "for df in df_list:\n",
    "    hr_positions = np.flatnonzero(df['heart_rate'].notna())\n",
    "    if len(hr_positions) < 2:\n",
    "        continue\n",
    "    first_idx, second_idx = hr_positions[0], hr_positions[1]\n",
    "    hr_1 = df['heart_rate'].iat[first_idx]\n",
    "    hr_2 = df['heart_rate'].iat[second_idx]\n",
    "    for pos in hr_positions[2:]:\n",
    "        fill_val = (hr_1 + hr_2) / 2\n",
    "        df.loc[first_idx + 1 : second_idx, 'heart_rate'] = (\n",
    "            df.loc[first_idx + 1 : second_idx, 'heart_rate'].fillna(fill_val)\n",
    "        )\n",
    "        first_idx, second_idx = second_idx, pos\n",
    "        hr_1, hr_2 = hr_2, df['heart_rate'].iat[pos]\n",
    "    df.loc[second_idx + 1 :, 'heart_rate'] = (\n",
    "        df.loc[second_idx + 1 :, 'heart_rate'].fillna(hr_2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting with Temporal Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Создание df_all с учетом деления на людей и временной согласованности:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24]\n",
    "\n",
    "subjects = {\n",
    "    1: [person1, person1_additional],\n",
    "    2: [person2],\n",
    "    3: [person3],\n",
    "    4: [person4],\n",
    "    5: [person5, person5_additional],\n",
    "    6: [person6, person6_additional],\n",
    "    7: [person7],\n",
    "    8: [person8, person8_additional],\n",
    "    9: [person9, person9_additional]\n",
    "}\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "for subject_id, dfs in subjects.items():\n",
    "    df_subject = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    df_subject['subject_id'] = subject_id\n",
    "    df_all = pd.concat([df_all, df_subject], axis=0, ignore_index=True)\n",
    "\n",
    "df_all = df_all.drop(columns=col_for_drop)\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all[df_all[\"activity_id\"] != 0]\n",
    "\n",
    "# Проверка активностей:\n",
    "unique_activities = sorted(df_all['activity_id'].unique())\n",
    "print(f\"Unique activities in combined dataset: {unique_activities}\")\n",
    "if set(unique_activities) == set(all_classes):\n",
    "    print(\"All required classes are present in the combined dataset.\")\n",
    "else:\n",
    "    print(f\"Warning: Missing classes {set(all_classes) - set(unique_activities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Мини-EDA для датасета:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Размер датасета:\", df_all.shape)\n",
    "print(\"\\nПервые 5 строк:\")\n",
    "print(df_all.head())\n",
    "print(\"\\nТипы данных:\")\n",
    "print(df_all.dtypes)\n",
    "\n",
    "numeric_cols = df_all.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('activity_id')\n",
    "numeric_cols.remove('subject_id')\n",
    "categorical_cols = ['activity_id', 'subject_id']\n",
    "print(\"\\nЧисловые признаки:\", numeric_cols)\n",
    "print(\"Категориальные признаки:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица корреляций для числовых признако\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df_all[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()\n",
    "\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "high_corr = corr_pairs[(abs(corr_pairs) > 0.7) & (abs(corr_pairs) < 1)].sort_values(ascending=False)\n",
    "print(\"\\nВысокие корреляции (|corr| > 0.7):\")\n",
    "print(high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение activity_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='activity_id', data=df_all, order=sorted(df_all['activity_id'].unique()))\n",
    "plt.title('Distribution of Activity Classes')\n",
    "plt.xlabel('Activity ID')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Распределение activity_id по субъектам\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='activity_id', hue='subject_id', data=df_all, order=sorted(df_all['activity_id'].unique()))\n",
    "plt.title('Activity Distribution by Subject')\n",
    "plt.xlabel('Activity ID')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Subject ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Доля классов\n",
    "class_counts = df_all['activity_id'].value_counts(normalize=True)\n",
    "print(\"\\nДоля каждого класса activity_id:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_cols = [col for col in numeric_cols if 'accel' in col and 'hand' in col]\n",
    "print(\"Выбранные признаки для анализа динамики:\", accel_cols)\n",
    "\n",
    "subject_1 = df_all[df_all['subject_id'] == 1].copy()\n",
    "subject_1['time'] = subject_1.index\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for col in accel_cols:\n",
    "    sns.lineplot(x='time', y=col, hue='activity_id', data=subject_1, alpha=0.5)\n",
    "plt.title('Temporal Dynamics of Hand Accelerometer Features (Subject 1)')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.legend(title='Activity ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot для топ-признаков\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, col in enumerate(top_numeric_cols[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(x='activity_id', y=col, data=df_all)\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Violin Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, col in enumerate(top_numeric_cols[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.violinplot(x='activity_id', y=col, data=df_all)\n",
    "    plt.title(f'Violin Plot of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "# Мозаичный плот для activity_id и subject_i\n",
    "plt.figure(figsize=(10, 6))\n",
    "mosaic(df_all, ['activity_id', 'subject_id'], title='Mosaic Plot of Activity ID vs Subject ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка аномалий (например, экстремальные значения)\n",
    "from scipy.stats import zscore\n",
    "z_scores = df_all[numeric_cols].apply(zscore)\n",
    "outliers = (z_scores.abs() > 3).sum()\n",
    "print(\"\\nКоличество выбросов (|z-score| > 3) по признакам:\")\n",
    "print(outliers[outliers > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Разделение на трейн и тест с учетом времени (классы активности, которые не попали - потом добавляются по маске и заново сортируются по людям и времени для сохранения порядка):\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_classes = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24]\n",
    "\n",
    "# Убедимся, что df_all отсортирован по времени\n",
    "df_all = df_all.sort_values(by=['subject_id', 'timestamp'])\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "# Сортируем subject_id по первой временной метке\n",
    "subject_order = df_all.groupby('subject_id')['timestamp'].min().sort_values().index\n",
    "\n",
    "for subject_id in subject_order:\n",
    "    df_subject = df_all[df_all['subject_id'] == subject_id]\n",
    "    split_idx = int(len(df_subject) * 0.8)\n",
    "    if split_idx == 0 or split_idx >= len(df_subject):\n",
    "        print(f\"Warning: Subject {subject_id} has insufficient data for splitting.\")\n",
    "        continue\n",
    "    train_part = df_subject.iloc[:split_idx]\n",
    "    test_part = df_subject.iloc[split_idx:]\n",
    "    train_df = pd.concat([train_df, train_part], axis=0, ignore_index=True)\n",
    "    test_df = pd.concat([test_df, test_part], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
    "train_classes = sorted(train_df['activity_id'].unique())\n",
    "test_classes = sorted(test_df['activity_id'].unique())\n",
    "print(f\"Train classes: {train_classes}\")\n",
    "print(f\"Test classes: {test_classes}\")\n",
    "\n",
    "missing_train = set(all_classes) - set(train_classes)\n",
    "missing_test = set(all_classes) - set(test_classes)\n",
    "if missing_train or missing_test:\n",
    "    print(f\"Warning: Missing classes in train: {missing_train}, test: {missing_test}\")\n",
    "    for activity in missing_train.union(missing_test):\n",
    "        activity_data = df_all[df_all['activity_id'] == activity]\n",
    "        if len(activity_data) > 0:\n",
    "            split_idx = int(len(activity_data) * 0.8)\n",
    "            if split_idx == 0:\n",
    "                split_idx = 1\n",
    "            if split_idx >= len(activity_data):\n",
    "                split_idx = len(activity_data) - 1\n",
    "            train_part = activity_data.iloc[:split_idx]\n",
    "            test_part = activity_data.iloc[split_idx:]\n",
    "            train_df = pd.concat([train_df, train_part], axis=0, ignore_index=True)\n",
    "            test_df = pd.concat([test_df, test_part], axis=0, ignore_index=True)\n",
    "\n",
    "# Сортируем итоговые наборы данных по subject_id, а затем по времен\n",
    "train_df = train_df.sort_values(by=['subject_id', 'timestamp'])\n",
    "test_df = test_df.sort_values(by=['subject_id', 'timestamp'])\n",
    "\n",
    "train_classes = sorted(train_df['activity_id'].unique())\n",
    "test_classes = sorted(test_df['activity_id'].unique())\n",
    "print(f\"Final train classes: {train_classes}\")\n",
    "print(f\"Final test classes: {test_classes}\")\n",
    "if set(train_classes) == set(test_classes) == set(all_classes):\n",
    "    print(\"All classes are present in both train and test sets.\")\n",
    "else:\n",
    "    print(f\"Warning: Still missing classes in train: {set(all_classes) - set(train_classes)}, test: {set(all_classes) - set(test_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df[\"sum_temperature\"] = (\n",
    "        df[\"IMU_hand_temperature\"] +\n",
    "        df[\"IMU_chest_temperature\"] +\n",
    "        df[\"IMU_ankle_temperature\"]\n",
    "    )\n",
    "    df[\"sum_hand_acceleration\"] = (\n",
    "        df[\"IMU_hand_3D_acceleration_data_16g_1\"] +\n",
    "        df[\"IMU_hand_3D_acceleration_data_16g_2\"] +\n",
    "        df[\"IMU_hand_3D_acceleration_data_16g_3\"]\n",
    "    )\n",
    "    df[\"sum_chest_acceleration\"] = (\n",
    "        df[\"IMU_chest_3D_acceleration_data_16g_1\"] +\n",
    "        df[\"IMU_chest_3D_acceleration_data_16g_2\"] +\n",
    "        df[\"IMU_chest_3D_acceleration_data_16g_3\"]\n",
    "    )\n",
    "    df[\"sum_ankle_acceleration\"] = (\n",
    "        df[\"IMU_ankle_3D_acceleration_data_16g_1\"] +\n",
    "        df[\"IMU_ankle_3D_acceleration_data_16g_2\"] +\n",
    "        df[\"IMU_ankle_3D_acceleration_data_16g_3\"]\n",
    "    )\n",
    "    df[\"heart_rate_rolling_mean\"] = df[\"heart_rate\"].rolling(window=5, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"activity_id\", \"subject_id\"])\n",
    "y_train = train_df[\"activity_id\"]\n",
    "X_test = test_df.drop(columns=[\"activity_id\", \"subject_id\"])\n",
    "y_test = test_df[\"activity_id\"]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Preparation for HMM (пока не нужно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(X, y, window_size=100):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    X_values = X.values if hasattr(X, 'values') else X\n",
    "    y_values = np.asarray(y)\n",
    "    \n",
    "    for i in range(0, len(X) - window_size + 1, window_size):\n",
    "        sequences.append(X_values[i:i + window_size])\n",
    "        labels.append(np.bincount(y_values[i:i + window_size].astype(int)).argmax())\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train_enc)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test_enc)\n",
    "\n",
    "print(f\"Train sequences: {X_train_seq.shape}, Test sequences: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Обучение моделек на подвыборке датасета:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### XGBoost:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Подвыборка 30% тренировочных данных для ускорения\n",
    "X_train_sub, y_train_sub = resample(X_train, y_train, n_samples=int(0.3 * len(X_train)), random_state=42)\n",
    "\n",
    "# Кодирование меток\n",
    "train_classes = np.unique(y_train_sub)\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "\n",
    "# Фильтрация тестового набора (только классы из тренировочного)\n",
    "test_mask = np.isin(y_test, train_classes)\n",
    "X_test_filt = X_test[test_mask]\n",
    "y_test_filt = y_test[test_mask]\n",
    "\n",
    "# Пайплайн с масштабированием и моделью\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        n_estimators=100,  # Уменьшено для скорости\n",
    "        learning_rate=0.1,\n",
    "        max_depth=10,  # Уменьшено\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение\n",
    "pipeline.fit(X_train_sub, y_train_enc)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_enc = pipeline.predict(X_test_filt)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Оценка\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test_filt, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### XGBoost Random Search:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Подвыборка 30% тренировочных данных\n",
    "X_train_sub, y_train_sub = resample(X_train, y_train, n_samples=int(0.3 * len(X_train)), random_state=42)\n",
    "\n",
    "# Кодирование меток\n",
    "train_classes = np.unique(y_train_sub)\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "\n",
    "# Фильтрация тестового набора\n",
    "mask = np.isin(y_test, train_classes)\n",
    "X_test_filt = X_test[mask]\n",
    "y_test_filt = y_test[mask]\n",
    "\n",
    "# Модель\n",
    "model = XGBClassifier(objective='multi:softprob', random_state=42)\n",
    "\n",
    "# Упрощенный поиск гиперпараметров\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Стохастический поиск\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=4,  # Меньше итераций\n",
    "    scoring='accuracy',\n",
    "    cv=2,  # Меньше фолдов\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "random_search.fit(X_train_sub, y_train_enc)\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_enc = best_model.predict(X_test_filt)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Оценка\n",
    "print(\"XGBoost Random Search Classification Report:\")\n",
    "print(classification_report(y_test_filt, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Random Forest:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Подвыборка 30% тренировочных данных\n",
    "X_train_sub, y_train_sub = resample(X_train, y_train, n_samples=int(0.3 * len(X_train)), random_state=42)\n",
    "\n",
    "# Кодирование меток\n",
    "le = LabelEncoder().fit(all_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Модель\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Уменьшено\n",
    "    max_depth=8,  # Уменьшено\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "model.fit(X_train_sub, y_train_enc)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_enc = model.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Оценка\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Важность признаков\n",
    "importances = model.feature_importances_\n",
    "features = X_train.columns\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(features, importances)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Logistic Regression:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Подвыборка 30% тренировочных данных\n",
    "X_train_sub, y_train_sub = resample(X_train, y_train, n_samples=int(0.3 * len(X_train)), random_state=42)\n",
    "\n",
    "# Масштабирование\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Кодирование меток\n",
    "le = LabelEncoder().fit(all_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Модель\n",
    "model = LogisticRegression(\n",
    "    max_iter=200,  # Уменьшено\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "model.fit(X_train_scaled, y_train_enc)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_enc = model.predict(X_test_scaled)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Оценка\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Важность признаков\n",
    "coefficients = np.abs(model.coef_).mean(axis=0)\n",
    "features = X_train.columns\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(features, coefficients)\n",
    "plt.xlabel(\"Coefficient Magnitude\")\n",
    "plt.title(\"Feature Importances (Logistic Regression)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### CatBoost:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Подвыборка 30% тренировочных данных\n",
    "X_train_sub, y_train_sub = resample(X_train, y_train, n_samples=int(0.3 * len(X_train)), random_state=42)\n",
    "\n",
    "# Кодирование меток\n",
    "le = LabelEncoder().fit(all_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Модель\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,  # Уменьшено\n",
    "    depth=4,  # Уменьшено\n",
    "    learning_rate=0.1,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "model.fit(X_train_sub, y_train_enc)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_enc = model.predict(X_test).flatten().astype(int)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Оценка\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Важность признаков\n",
    "importances = model.get_feature_importance()\n",
    "features = X_train.columns\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(features, importances)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importances (CatBoost)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### XGBoost + HMM:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Функция ordered_unique\n",
    "def ordered_unique(x):\n",
    "    \"\"\" Return unique elements, maintaining order of appearance \"\"\"\n",
    "    return x[np.sort(np.unique(x, return_index=True)[1])]\n",
    "\n",
    "# Реализация HMM\n",
    "class HMM:\n",
    "    def __init__(self, startprob=None, emissionprob=None, transmat=None, n_iter=100, random_state=None):\n",
    "        self.startprob = startprob\n",
    "        self.emissionprob = emissionprob\n",
    "        self.transmat = transmat\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.labels = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"HMM Model:\\n\"\n",
    "            f\"{ {'prior': self.startprob, 'emission': self.emissionprob, 'transition': self.transmat} }\"\n",
    "        )\n",
    "\n",
    "    def fit(self, Y_pred, Y_true, groups=None):\n",
    "        self.labels = np.unique(Y_true)\n",
    "        self.startprob = self.compute_prior(Y_true, self.labels, uniform=False)  # Неравномерные вероятности\n",
    "        self.emissionprob = self.compute_emission(Y_pred, Y_true, self.labels)\n",
    "        self.transmat = self.compute_transition(Y_true, self.labels, groups)\n",
    "\n",
    "    def predict(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "\n",
    "        if groups is None:\n",
    "            Y_vit, _ = self._viterbi(Y, params)\n",
    "        else:\n",
    "            Y_vit = np.concatenate([\n",
    "                self._viterbi(Y[groups == g], params)[0]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return Y_vit\n",
    "\n",
    "    def predict_proba(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "        if groups is None:\n",
    "            _, probs = self._viterbi(Y, params, True)\n",
    "        else:\n",
    "            probs = np.concatenate([\n",
    "                self._viterbi(Y[groups == g], params, True)[1]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return probs\n",
    "\n",
    "    def optimise(self, **kwargs):\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_transition(Y, labels=None, groups=None):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y)\n",
    "\n",
    "        def _compute_transition(Y):\n",
    "            transition = np.vstack([\n",
    "                np.sum(Y[1:][(Y == label)[:-1]].reshape(-1, 1) == labels, axis=0)\n",
    "                for label in labels\n",
    "            ])\n",
    "            return transition\n",
    "\n",
    "        if groups is None:\n",
    "            transition = _compute_transition(Y)\n",
    "        else:\n",
    "            transition = sum((\n",
    "                _compute_transition(Y[groups == g])\n",
    "                for g in ordered_unique(groups)\n",
    "            ))\n",
    "\n",
    "        # Laplace smoothing\n",
    "        transition = transition + 1\n",
    "        transition = transition / np.sum(transition, axis=1).reshape(-1, 1)\n",
    "        return transition\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_emission(Y_score, Y_true, labels=None):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y_true)\n",
    "\n",
    "        if Y_score.ndim == 1:\n",
    "            Y_pred = np.vstack([\n",
    "                (Y_score == label).astype('float')[:, None]\n",
    "                for label in labels\n",
    "            ])\n",
    "        else:\n",
    "            Y_pred = Y_score\n",
    "\n",
    "        emission = np.vstack(\n",
    "            [np.mean(Y_pred[Y_true == label], axis=0) for label in labels]\n",
    "        )\n",
    "        return emission\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prior(Y_true, labels=None, uniform=True):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y_true)\n",
    "\n",
    "        if uniform:\n",
    "            prior = np.ones(len(labels)) / len(labels)\n",
    "        else:\n",
    "            prior = np.mean(Y_true.reshape(-1, 1) == labels, axis=0)\n",
    "        return prior\n",
    "\n",
    "    def _viterbi(self, Y, hmm_params, return_probs=False):\n",
    "        if len(Y) == 0:\n",
    "            return np.empty_like(Y), np.array([])\n",
    "\n",
    "        def log(x):\n",
    "            SMALL_NUMBER = 1e-16\n",
    "            return np.log(x + SMALL_NUMBER)\n",
    "\n",
    "        prior = hmm_params['prior']\n",
    "        emission = hmm_params['emission']\n",
    "        transition = hmm_params['transition']\n",
    "        labels = hmm_params['labels']\n",
    "\n",
    "        nobs = len(Y)\n",
    "        nlabels = len(labels)\n",
    "\n",
    "        if Y.ndim == 1:\n",
    "            Y = np.where(Y.reshape(-1, 1) == labels)[1]\n",
    "        else:\n",
    "            Y = Y\n",
    "\n",
    "        probs = np.zeros((nobs, nlabels))\n",
    "        probs[0, :] = log(prior) + log(emission[:, Y[0] if Y.ndim == 1 else np.argmax(Y[0])])\n",
    "        for j in range(1, nobs):\n",
    "            for i in range(nlabels):\n",
    "                probs[j, i] = np.max(\n",
    "                    log(emission[i, Y[j] if Y.ndim == 1 else np.argmax(Y[j])]) +\n",
    "                    log(transition[:, i]) +\n",
    "                    probs[j - 1, :])\n",
    "        viterbi_path = np.zeros_like(Y, dtype=int) if Y.ndim == 1 else np.zeros(len(Y), dtype=int)\n",
    "        viterbi_path[-1] = np.argmax(probs[-1, :])\n",
    "        for j in reversed(range(nobs - 1)):\n",
    "            viterbi_path[j] = np.argmax(\n",
    "                log(transition[:, viterbi_path[j + 1]]) +\n",
    "                probs[j, :])\n",
    "\n",
    "        viterbi_path = labels[viterbi_path]\n",
    "\n",
    "        if return_probs:\n",
    "            return viterbi_path, np.exp(probs)\n",
    "        return viterbi_path, None\n",
    "\n",
    "# Подвыборка 50% тренировочных данных с сохранением временного порядка\n",
    "n_samples = int(0.5 * len(X_train))\n",
    "X_train_sub = X_train.iloc[:n_samples].copy()\n",
    "y_train_sub = y_train.iloc[:n_samples].copy()\n",
    "\n",
    "# Проверка наличия всех классов и добавление отсутствующих\n",
    "all_classes = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24]\n",
    "missing_classes = set(all_classes) - set(np.unique(y_train_sub))\n",
    "if missing_classes:\n",
    "    print(f\"Warning: Missing classes in training subset: {missing_classes}\")\n",
    "    for cls in missing_classes:\n",
    "        cls_indices = np.where(y_train == cls)[0]\n",
    "        if len(cls_indices) > 0:\n",
    "            first_idx = cls_indices[0]\n",
    "            X_train_sub = pd.concat([X_train_sub, X_train.iloc[[first_idx]]], ignore_index=True)\n",
    "            y_train_sub = pd.concat([y_train_sub, y_train.iloc[[first_idx]]], ignore_index=True)\n",
    "\n",
    "# Кодирование меток\n",
    "train_classes = np.unique(y_train_sub)\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Вычисление весов классов\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_enc), y=y_train_enc)\n",
    "sample_weights = np.array([class_weights[cls] for cls in y_train_enc])\n",
    "\n",
    "# Пайплайн XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        n_estimators=200,  # Увеличено\n",
    "        learning_rate=0.05,  # Уменьшено\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение с весами классов\n",
    "pipeline.fit(X_train_sub, y_train_enc, classifier__sample_weight=sample_weights)\n",
    "y_pred_enc = pipeline.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "# Сглаживание вероятностей\n",
    "epsilon = 1e-5\n",
    "y_train_proba = pipeline.predict_proba(X_train_sub) * (1 - 18 * epsilon) + epsilon\n",
    "y_pred_proba = pipeline.predict_proba(X_test) * (1 - 18 * epsilon) + epsilon\n",
    "\n",
    "# HMM для сглаживания предсказаний\n",
    "hmm_model = HMM(n_iter=100, random_state=42)\n",
    "hmm_model.fit(y_train_proba, y_train_enc)\n",
    "\n",
    "# Корректировка предсказаний\n",
    "y_pred_hmm = hmm_model.predict(y_pred_proba)\n",
    "y_pred_hmm = le.inverse_transform(y_pred_hmm)\n",
    "\n",
    "# Оценка\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"XGBoost + HMM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_hmm, zero_division=0))\n",
    "\n",
    "# Сохранение метрик\n",
    "metrics = {\n",
    "    'XGBoost': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro')\n",
    "    },\n",
    "    'XGBoost_HMM': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_hmm),\n",
    "        'f1_macro': f1_score(y_test, y_pred_hmm, average='macro')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Вывод матриц HMM для отладки\n",
    "print(hmm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Функция ordered_unique\n",
    "def ordered_unique(x):\n",
    "    \"\"\"Return unique elements, maintaining order of appearance\"\"\"\n",
    "    return np.unique(x)[np.argsort(np.unique(x, return_index=True)[1])]\n",
    "\n",
    "# Реализация HMM\n",
    "class HMM:\n",
    "    def __init__(self, startprob=None, emissionprob=None, transmat=None, n_iter=100, random_state=None):\n",
    "        self.startprob = startprob\n",
    "        self.emissionprob = emissionprob\n",
    "        self.transmat = transmat\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.labels = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"HMM Model:\\n\"\n",
    "            f\"{ {'startprob': self.startprob, 'emission': self.emissionprob, 'transition': self.transmat} }\"\n",
    "        )\n",
    "\n",
    "    def fit(self, Y_pred, Y_true, groups=None):\n",
    "        self.labels = np.sort(np.unique(Y_true))\n",
    "        self.startprob = self.compute_probability(Y_true, self.labels, uniform=False)\n",
    "        self.emissionprob = self.compute_emission_probability(Y_pred, Y_true, self.labels)\n",
    "        self.transmat = self.compute_transition_probability(Y_true, self.labels, groups)\n",
    "\n",
    "    def compute_probability(self, Y_true, labels=None, uniform=False):\n",
    "        if labels is None:\n",
    "            labels = np.sort(np.unique(Y_true))\n",
    "\n",
    "        if uniform:\n",
    "            probability = np.ones(self.labels(Y_true)) / len(Y_true)\n",
    "        else:\n",
    "            probability = np.mean(Y_true.reshape(-1, 1) == labels, axis=0)\n",
    "        return probability\n",
    "\n",
    "    def compute_emission_probability(self, Y_score, Y_true, labels=None):\n",
    "        if labels is None:\n",
    "            labels = np.sort(np.unique(Y_true))\n",
    "\n",
    "        if Y_score.ndim == 1:\n",
    "            Y_pred = np.vstack([\n",
    "                (Y_score == labels).astype('float')[:, None]\n",
    "            for label in labels])\n",
    "        else:\n",
    "            Y_pred = Y_score\n",
    "\n",
    "        emission = np.vstack([\n",
    "            np.mean(Y_pred[Y_true == label], axis=0) for label in labels\n",
    "        ])\n",
    "        return emission\n",
    "\n",
    "    def compute_transition_probability(self, Y, labels=None, groups=None):\n",
    "        if labels is None:\n",
    "            labels = np.sort(np.unique(Y))\n",
    "\n",
    "        def _compute_transition(Y):\n",
    "            transition = np.vstack([\n",
    "                np.sum(Y[1:][(Y == label)[:-1]].reshape(-1, 1) == labels, axis=0)\n",
    "                for label in labels\n",
    "            ])\n",
    "            return transition\n",
    "\n",
    "        if groups is None:\n",
    "            transition = _compute_transition(Y)\n",
    "        else:\n",
    "            transition = sum((\n",
    "                _compute_transition(Y[groups == g])\n",
    "                for g in ordered_unique(groups)\n",
    "            ))\n",
    "\n",
    "        # Laplace smoothing\n",
    "        transition = transition + 1\n",
    "        transition = transition / np.sum(transition, axis=1).reshape(-1, 1)\n",
    "        return transition\n",
    "\n",
    "    def predict(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "\n",
    "        if groups is None:\n",
    "            Y_vit, _ = self._viterbi(Y, params)\n",
    "        else:\n",
    "            Y_vit = np.concatenate([\n",
    "                self._viterbi(Y[groups == g], params)[0]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return Y_vit\n",
    "\n",
    "    def predict_probability(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "        if groups is None:\n",
    "            _, probs = self._viterbi(Y, params, True)\n",
    "        else:\n",
    "            probs = np.concatenate([\n",
    "                self._viterbi(Y[groups == g], params, True)[1]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return probs\n",
    "\n",
    "    def optimise(self, **kwargs):\n",
    "        return\n",
    "\n",
    "    def _viterbi(self, Y, hmm_params, return_probs=False):\n",
    "        if len(Y) == 0:\n",
    "            return np.empty_like(Y), np.array([])\n",
    "\n",
    "        def log(x):\n",
    "            SMALL_NUMBER = 1e-16\n",
    "            return np.log(x + SMALL_NUMBER)\n",
    "\n",
    "        prior = hmm_params['prior']\n",
    "        emission = hmm_params['emission']\n",
    "        transition = hmm_params['transition']\n",
    "        labels = hmm_params['labels']\n",
    "\n",
    "        nobs = len(Y)\n",
    "        nlabels = len(labels)\n",
    "\n",
    "        if Y.ndim == 1:\n",
    "            Y = np.where(Y.reshape(-1, 1) == labels)[1]\n",
    "        else:\n",
    "            Y = Y\n",
    "\n",
    "        probs = np.zeros((nobs, nlabels))\n",
    "        probs[0, :] = log(prior) + log(emission[:, Y[0] if Y.ndim == 1 else np.argmax(Y[0])])\n",
    "        for j in range(1, nobs):\n",
    "            for i in range(nlabels):\n",
    "                probs[j, i] = np.max(\n",
    "                    log(emission[i, Y[j] if Y.ndim == 1 else np.argmax(Y[j])]) +\n",
    "                    log(transition[:, i]) +\n",
    "                    probs[j - 1, :])\n",
    "        viterbi_path = np.zeros_like(Y, dtype=int) if Y.ndim == 1 else np.zeros(len(Y), dtype=int)\n",
    "        viterbi_path[-1] = np.argmax(probs[-1, :])\n",
    "        for j in reversed(range(nobs - 1)):\n",
    "            viterbi_path[j] = np.argmax(\n",
    "                log(transition[:, viterbi_path[j + 1]]) +\n",
    "                probs[j, :])\n",
    "\n",
    "        viterbi_path = labels[viterbi_path]\n",
    "\n",
    "        if return_probs:\n",
    "            return viterbi_path, np.exp(probs)\n",
    "        return viterbi_path, None\n",
    "\n",
    "# Кодирование меток\n",
    "le = LabelEncoder().fit(np.unique(y_train))\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Вычисление весов классов\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_enc), y=y_train_enc)\n",
    "sample_weights = np.array([class_weights[cls] for cls in y_train_enc])\n",
    "\n",
    "# Пайплайн XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        n_estimators=300,  # Увеличено для полной выборки\n",
    "        learning_rate=0.05,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение на полной выборке\n",
    "pipeline.fit(X_train, y_train_enc, classifier__sample_weight=sample_weights)\n",
    "y_pred_enc = pipeline.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "# Сглаживание вероятностей\n",
    "epsilon = 1e-5\n",
    "y_train_proba = pipeline.predict_proba(X_train) * (1 - len(np.unique(y_train_enc)) * epsilon) + epsilon\n",
    "y_pred_proba = pipeline.predict_proba(X_test) * (1 - len(np.unique(y_train_enc)) * epsilon) + epsilon\n",
    "\n",
    "# HMM для сглаживания предсказаний\n",
    "hmm_model = HMM(n_iter=100, random_state=42)\n",
    "hmm_model.fit(y_train_proba, y_train_enc)\n",
    "\n",
    "# Корректировка предсказаний\n",
    "y_pred_hmm = hmm_model.predict(y_pred_proba)\n",
    "y_pred_hmm = le.inverse_transform(y_pred_hmm)\n",
    "\n",
    "# Оценка\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"XGBoost + HMM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_hmm, zero_division=0))\n",
    "\n",
    "# Сохранение метрик\n",
    "metrics = {\n",
    "    'XGBoost': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro')\n",
    "    },\n",
    "    'XGBoost_HMM': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_hmm),\n",
    "        'f1_macro': f1_score(y_test, y_pred_hmm, average='macro')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Вывод матриц HMM для отладки\n",
    "print(hmm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def ordered_unique(x):\n",
    "    \"\"\" Return unique elements, maintaining order of appearance \"\"\"\n",
    "    return x[np.sort(np.unique(x, return_index=True)[1])]\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, startprob=None, emissionprob=None, transmat=None, n_iter=100, random_state=None):\n",
    "        self.startprob = startprob\n",
    "        self.emissionprob = emissionprob\n",
    "        self.transmat = transmat\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.labels = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"HMM Model:\\n\"\n",
    "            f\"{ {'prior': self.startprob, 'emission': self.emissionprob, 'transition': self.transmat} }\"\n",
    "        )\n",
    "\n",
    "    def fit(self, Y_pred, Y_true, groups=None):\n",
    "        self.labels = np.unique(Y_true)\n",
    "        self.startprob = self.compute_prior(Y_true, self.labels, uniform=False)\n",
    "        self.emissionprob = self.compute_emission(Y_pred, Y_true, self.labels)\n",
    "        self.transmat = self.compute_transition(Y_true, self.labels, groups)\n",
    "\n",
    "    def predict(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "        if groups is None:\n",
    "            Y_vit, _ = self._viterbi(np.argmax(Y, axis=1), params)\n",
    "        else:\n",
    "            Y_vit = np.concatenate([\n",
    "                self._viterbi(np.argmax(Y[groups == g], axis=1), params)[0]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return Y_vit\n",
    "\n",
    "    def predict_proba(self, Y, groups=None):\n",
    "        params = {\n",
    "            'prior': self.startprob,\n",
    "            'emission': self.emissionprob,\n",
    "            'transition': self.transmat,\n",
    "            'labels': self.labels,\n",
    "        }\n",
    "        if groups is None:\n",
    "            _, probs = self._viterbi(np.argmax(Y, axis=1), params, True)\n",
    "        else:\n",
    "            probs = np.concatenate([\n",
    "                self._viterbi(np.argmax(Y[groups == g], axis=1), params, True)[1]\n",
    "                for g in ordered_unique(groups)\n",
    "            ])\n",
    "        return probs\n",
    "\n",
    "    def optimise(self, **kwargs):\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_transition(Y, labels=None, groups=None):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y)\n",
    "        def _compute_transition(Y):\n",
    "            transition = np.vstack([\n",
    "                np.sum(Y[1:][(Y == label)[:-1]].reshape(-1, 1) == labels, axis=0)\n",
    "                for label in labels\n",
    "            ])\n",
    "            return transition\n",
    "        if groups is None:\n",
    "            transition = _compute_transition(Y)\n",
    "        else:\n",
    "            transition = sum((\n",
    "                _compute_transition(Y[groups == g])\n",
    "                for g in ordered_unique(groups)\n",
    "            ))\n",
    "        transition = transition + 1  # Laplace smoothing\n",
    "        transition = transition / np.sum(transition, axis=1).reshape(-1, 1)\n",
    "        return transition\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_emission(Y_score, Y_true, labels=None):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y_true)\n",
    "        Y_pred = np.argmax(Y_score, axis=1)  # Бинарные предсказания\n",
    "        emission = np.zeros((len(labels), len(labels)))\n",
    "        for i, true_label in enumerate(labels):\n",
    "            mask = (Y_true == true_label)\n",
    "            if np.sum(mask) > 0:\n",
    "                emission[i] = np.bincount(Y_pred[mask], minlength=len(labels)) / np.sum(mask)\n",
    "        return emission\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prior(Y_true, labels=None, uniform=True):\n",
    "        if labels is None:\n",
    "            labels = np.unique(Y_true)\n",
    "        if uniform:\n",
    "            prior = np.ones(len(labels)) / len(labels)\n",
    "        else:\n",
    "            prior = np.mean(Y_true.reshape(-1, 1) == labels, axis=0)\n",
    "        return prior\n",
    "\n",
    "    def _viterbi(self, Y, hmm_params, return_probs=False):\n",
    "        if len(Y) == 0:\n",
    "            return np.empty_like(Y), np.array([])\n",
    "        def log(x):\n",
    "            SMALL_NUMBER = 1e-16\n",
    "            return np.log(x + SMALL_NUMBER)\n",
    "        prior = hmm_params['prior']\n",
    "        emission = hmm_params['emission']\n",
    "        transition = hmm_params['transition']\n",
    "        labels = hmm_params['labels']\n",
    "        nobs = len(Y)\n",
    "        nlabels = len(labels)\n",
    "        probs = np.zeros((nobs, nlabels))\n",
    "        probs[0, :] = log(prior) + log(emission[:, Y[0]])\n",
    "        for j in range(1, nobs):\n",
    "            for i in range(nlabels):\n",
    "                probs[j, i] = np.max(\n",
    "                    log(emission[i, Y[j]]) +\n",
    "                    log(transition[:, i]) +\n",
    "                    probs[j - 1, :])\n",
    "        viterbi_path = np.zeros(nobs, dtype=int)\n",
    "        viterbi_path[-1] = np.argmax(probs[-1, :])\n",
    "        for j in reversed(range(nobs - 1)):\n",
    "            viterbi_path[j] = np.argmax(\n",
    "                log(transition[:, viterbi_path[j + 1]]) +\n",
    "                probs[j, :])\n",
    "        viterbi_path = labels[viterbi_path]\n",
    "        if return_probs:\n",
    "            return viterbi_path, np.exp(probs)\n",
    "        return viterbi_path, None\n",
    "\n",
    "# Используем полный набор данных\n",
    "X_train_sub = X_train.copy()\n",
    "y_train_sub = y_train.copy()\n",
    "\n",
    "# Проверка наличия всех классов (опционально, так как используем полный набор)\n",
    "all_classes = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 24]\n",
    "missing_classes = set(all_classes) - set(np.unique(y_train_sub))\n",
    "if missing_classes:\n",
    "    print(f\"Warning: Missing classes in training subset: {missing_classes}\")\n",
    "    for cls in missing_classes:\n",
    "        cls_indices = np.where(y_train == cls)[0]\n",
    "        if len(cls_indices) > 0:\n",
    "            first_idx = cls_indices[0]\n",
    "            X_train_sub = pd.concat([X_train_sub, X_train.iloc[[first_idx]]], ignore_index=True)\n",
    "            y_train_sub = pd.concat([y_train_sub, y_train.iloc[[first_idx]]], ignore_index=True)\n",
    "\n",
    "# Кодирование меток\n",
    "train_classes = np.unique(y_train_sub)\n",
    "le = LabelEncoder().fit(train_classes)\n",
    "y_train_enc = le.transform(y_train_sub)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Вычисление весов классов\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_enc), y=y_train_enc)\n",
    "sample_weights = np.array([class_weights[cls] for cls in y_train_enc])\n",
    "\n",
    "# Пайплайн XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('classifier', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Обучение с весами классов\n",
    "pipeline.fit(X_train_sub, y_train_enc, classifier__sample_weight=sample_weights)\n",
    "y_pred_enc = pipeline.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# Сглаживание вероятностей\n",
    "epsilon = 1e-5\n",
    "y_train_proba = pipeline.predict_proba(X_train_sub) * (1 - len(np.unique(y_train_enc)) * epsilon) + epsilon\n",
    "y_pred_proba = pipeline.predict_proba(X_test) * (1 - len(np.unique(y_train_enc)) * epsilon) + epsilon\n",
    "\n",
    "# HMM для сглаживания предсказаний\n",
    "hmm_model = HMM(n_iter=100, random_state=42)\n",
    "hmm_model.fit(y_train_proba, y_train_enc)\n",
    "\n",
    "# Корректировка предсказаний с учётом субъектов\n",
    "groups_test = test_df['subject_id'].values  # Предполагаем, что есть subject_id\n",
    "y_pred_hmm = hmm_model.predict(y_pred_proba, groups=groups_test)\n",
    "y_pred_hmm = le.inverse_transform(y_pred_hmm)\n",
    "\n",
    "# Оценка\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"XGBoost + HMM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_hmm, zero_division=0))\n",
    "\n",
    "# Сохранение метрик\n",
    "metrics = {\n",
    "    'XGBoost': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_macro': f1_score(y_test, y_pred, average='macro')\n",
    "    },\n",
    "    'XGBoost_HMM': {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_hmm),\n",
    "        'f1_macro': f1_score(y_test, y_pred_hmm, average='macro')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Вывод матриц HMM для отладки\n",
    "print(hmm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
